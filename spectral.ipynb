{
 "metadata": {
  "name": "",
  "signature": "sha256:0f0641749b8befb319a5d3d2a7cf3949a41ac4d4d49007013aac79fb6567df82"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "<span id=\"chap_spectral\"></span>\n",
      "Spectral network theory"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "...\n",
      "\n",
      "Matrices are a technique of linear algebra, used to manipulate a number of related linear equations simultaneously. In computer science they turn up a lot in graphics. It's not blindingly obvious that they should be appropriate for network science.\n",
      "\n",
      "The use of matrices for networks comes from their ability to manipulate a large group of related data simultaneously. It turns out that several matrix operations &ndash; notably addition, multiplication, and eigenvalue decomposition &ndash; have significance for networks. Other operations, by contrast, do not have obvious interpretations: one can't apply all of linear algebra blindly to networks. However, when one encounters a set of linear equations in several unknowns, or some other regular structure, it's natural to start thinking about a matrix representation. \n",
      "\n",
      "In this chapter we'll start by looking at the most basic use of matrices to represent adjacency, and see how they can be sued to simplify a number of operations. We'll then look at the Laplacian matrix and show how it captures some quite subtle features of a network."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "<span id=\"sec_matrix_algebra\"></span>\n",
      "Some matrix algebra"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Before we jump into the applications, it's perhaps as well to review the basic of matrix algebra and to present (without proof) some properties that may be unfamiliar.\n",
      "\n",
      "A matrix is an $n \\times m$ rectangular grid of (typically) numbers, for example:\n",
      "\n",
      "$$\n",
      "    \\mathbf{M} = \n",
      "    \\begin{pmatrix}\n",
      "        1 &  2 &  3 &  4 \\\\\n",
      "        5 &  6 &  7 &  8\\\\\n",
      "        9 & 10 & 11 & 12\n",
      "    \\end{pmatrix}\n",
      "$$\n",
      "\n",
      "is a $3 \\times 4$ rectangular matrix: the dimensions are always stated as $rows \\times columns$. We can index into a matrix using row and column numbers starting from 1 and denoted by subscripting, so $M_{2 \\, 3} = 7$ in this case.\n",
      "\n",
      "Notationally, we'll write whole matrices in bold (for example $\\mathbf{A}$) and elements as normal text (for example $A_{ij}$).\n",
      "\n",
      "We'll assume a familiarity with scalar and matrix arithmetic:\n",
      "\n",
      "\\begin{align*}\n",
      "    \\begin{pmatrix}\n",
      "        1 & 2 \\\\\n",
      "        3 & 4\n",
      "    \\end{pmatrix}\n",
      "    + \n",
      "    \\begin{pmatrix}\n",
      "        5 & 6 \\\\\n",
      "        7 & 8\n",
      "    \\end{pmatrix}\n",
      "    &=\n",
      "    \\begin{pmatrix}\n",
      "        1 + 5 & 2 + 6 \\\\\n",
      "        3 + 7 & 4 + 8\n",
      "    \\end{pmatrix}\n",
      "    \\\\\n",
      "    2\n",
      "    \\begin{pmatrix}\n",
      "        1 & 2 \\\\\n",
      "        3 & 4\n",
      "    \\end{pmatrix}\n",
      "    & =\n",
      "    \\begin{pmatrix}\n",
      "        2 \\times 1 & 2 \\times 2 \\\\\n",
      "        2 \\times 3 & 2 \\times 4\n",
      "    \\end{pmatrix}\n",
      "    \\\\\n",
      "    \\begin{pmatrix}\n",
      "        1 & 2 & 3\\\\\n",
      "        4 & 5 & 6\n",
      "    \\end{pmatrix}\n",
      "    \\begin{pmatrix}\n",
      "         7 &  8 \\\\\n",
      "         9 &  10 \\\\\n",
      "        11 & 12\n",
      "    \\end{pmatrix}\n",
      "    &=\n",
      "    \\begin{pmatrix}\n",
      "        1 \\times 7 + 2 \\times 9 + 3 \\times 11 & 1 \\times 8 + 2 \\times 10 + 3 \\times 11 \\\\  \n",
      "        4 \\times 7 + 5 \\times 9 + 6 \\times 11 & 4 \\times 8 + 5 \\times 10 + 6 \\times 11 \\\\  \n",
      "    \\end{pmatrix}\n",
      "\\end{align*}\n",
      "\n",
      "A *diagonal* matrix is a square $n \\times n$ matrix that is zero everywhere except along its main diagonal. An *identity* matrix is a diagonal matrix where all the entries on the main diagonal are 1:\n",
      "\n",
      "$$\n",
      "    \\mathbf{I}_3 = \\begin{pmatrix}\n",
      "                       1 & 0 & 0 \\\\\n",
      "                       0 & 1 & 0 \\\\\n",
      "                       0 & 0 & 1\n",
      "                   \\end{pmatrix}\n",
      "$$\n",
      "\n",
      "This has the property that multiplying any matrix by it leaves that matrix unchanged:\n",
      "\n",
      "$$\n",
      "    \\mathbf{I_3} \\begin{pmatrix}\n",
      "                     1 & 2 & 3 \\\\\n",
      "                     4 & 5 & 6 \\\\\n",
      "                     7 & 8 & 9\n",
      "                 \\end{pmatrix}\n",
      "    =\n",
      "    \\begin{pmatrix}\n",
      "        1 * 1 + 0 * 4 + 0 * 7 & 1 * 2 + 0 * 5 + 0 * 8 & 1 * 3 + 0 * 6 + 0 * 9 \\\\\n",
      "        0 * 1 + 1 * 4 + 0 * 7 & 0 * 2 + 1 * 5 + 0 * 8 & 0 * 3 + 1 * 6 + 0 * 9 \\\\\n",
      "        0 * 1 + 0 * 4 + 1 * 7 & 0 * 2 + 0 * 5 + 1 * 8 & 0 * 3 + 0 * 6 + 1 * 9 \\\\\n",
      "    \\end{pmatrix}\n",
      "    =\n",
      "    \\begin{pmatrix}\n",
      "        1 & 2 & 3 \\\\\n",
      "        4 & 5 & 6 \\\\\n",
      "        7 & 8 & 9\n",
      "    \\end{pmatrix}\n",
      "$$"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Eigenvalues, eigenvectors, and eigenspaces"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "...\n",
      "\n",
      "The *eigenvectors* $\\mathbf{v_i}$ and *eigenvalues* $\\lambda_i$ of a matrix $\\mathbf{M}$ are the solutions to the equation:\n",
      "\n",
      "$$\n",
      "    \\mathbf{M} \\mathbf{v_i} = \\lambda_i \\mathbf{v_i}\n",
      "$$\n",
      "\n",
      "An eigenvector of $\\mathbf{M}$ is a vector that, when multipled by $\\mathbf{M}$, is turned into a multiple of itself. That is to say, the direction of the eigenvector stays the same but its length is changed by some factor.\n",
      "\n",
      "Eigenvalues can in general be complex numbers, but the eigenvalues of *symmetric* matrices &ndash; matrices reflected about their main diagonal, so that $M_{ij} = M_{ji}$ &ndash; are always guaranteed to be real.\n",
      "\n",
      "We can place the eigenvalues (and their corresponding eigenvectors) into ascending order and number them from 1, with $\\lambda_1$ being the smallest eigenvalue, $\\lambda_2$ being the next smallest, and so on. It is also possible for a single eigenvalue to be *repeated* with different eigenvectors. We label the *distinct* eigenvalues $\\mu_1, \\mu_2$ and so on.\n",
      "\n",
      "A set of eigenvectors is *orthogonal* if no member of the set can be expressed in terms of the others. The set of orthogonal eigenvectors of $\\mathbf{M}$ forms the *eigenspace* of the matrix. "
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "<span id=\"sec_adjacency_matrix\"></span>\n",
      "The adjacency matrix"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "So far we have discussed networks \"as a whole\", in the sense that we have never needed to really get to grips with exactly how nodes and edges are connected. it has been sufficient that we know some high-level, often global, properties: the degree distribution, the mean degree, the connectedness (or not). The only time the detail has concerned us is when we are creating network instances to study, at which point we need to specify all the details.\n",
      "\n",
      "This may give the impression that the high-level properties are always readily available, but in fact the reverse is often true: in real-world networks we often have ready access to the details of the network, and have to calculate the summary statistics and so forth. In thinking about computing with networks, we also have to consider how to represent the nodes and edges in a storage- and computation-efficient structure from which we can perform the necessary calculations and simulations.\n",
      "\n",
      "The basic information about a network are its nodes and edges, and how the edges connect the nodes.\n",
      "\n",
      "...\n",
      "\n",
      "The *adjacency matrix* captures node and edge information in a single structure. It is defined very simply. For a network with $N$ nodes, we first number the nodes $(1 \\ldots N)$. We then define an $N \\times N$ adjacency matrix $\\mathbf{A}$ which has the value $A_{ij} = 1$ if there is an edge between node $i$ and node $j$, and $A_{ij} = 0$ otherwise.\n",
      "\n",
      "A few properties should be immediately apparent. Firstly, the main diagonal of $\\mathbf{A}$ will be all zeros, since a node is not adjacent to itself. Secondly, for an undirected network, the adjacency matrix will be symmetric, since if there is an undirected edge between $i$ and $j$ that edge can be regarded as \"going the other way\" from $j$ to $i$."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Node degree"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Once we have the adjacency matrix $\\mathbf{A}$, we can immediately work out the degree of a node. The degree of a node $i$ is simply the sum of the values in row $i$:\n",
      "\n",
      "$$\n",
      "    k_i = \\sum_j A_{ij}\n",
      "$$"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "The adjacency matrix as a network representation"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "..."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "The adjacency matrix from code"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "`NetworkX` has a number of functions built-in to aid computations with graph spectra. These operations work well with `numpy`'s matrix operations and are thus quite efficient. We can, for example, extract the adjacency matrix of a network and then use `numpy`'s array operations to compute the mean degree, checking that is conforms to `NetworkX`'s own calculation and with [what we'd expect theoretically for an ER network](er-network.ipynb#sec_er_mean_degree_maths):"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import networkx as nx\n",
      "import numpy as np\n",
      "\n",
      "# create a sample ER network\n",
      "(N, phi) = (1000, 0.001)\n",
      "er = nx.erdos_renyi_graph(N, phi)\n",
      "\n",
      "# convert to an adjancy matrix in numpy\n",
      "A = nx.to_numpy_matrix(er)\n",
      "\n",
      "# compute the mean degree as the average of the matrix rows...\n",
      "kmean_adj = np.average(np.sum(A, axis = 0))\n",
      "\n",
      "# ...and directly using NetworkX's degree() function\n",
      "kmean_nx = (np.sum(np.array(er.degree().values())) + 0.0) / N\n",
      "\n",
      "# compare the values computed experimentally and from theory\n",
      "print \"Adjacency {adj}, native {nat}, expected {ex}\".format(adj = kmean_adj,\n",
      "                                                            nat = kmean_nx,\n",
      "                                                            ex = N * phi)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Adjacency 1.108, native 1.108, expected 1.0\n"
       ]
      }
     ],
     "prompt_number": 40
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We've made use of `numpy`'s array-handling capabilities here, notably the functions that can compute across entire arrays or array rows (or columns) in a single statement, without needing loops. The expression `np.sum(A, axis = 0)` computes the sum function along the first (rows) axis, giving a column vector of degrees. The `np.average()` expression then computes the average of this vector, giving the mean degree. We could re-code both of these operations as explicit loops, but the array-processing functions are considerably faster and more expressive. Gettig to use these functions involves getting the adjacency matrix as a `numpy` array, which is what `nx.to_numpy_array()` does. (There is another `NetworkX` function, `nx.adjacency_matrix()`, which returns a \"sparse\" array that's more space-efficient but harder to compute with.)"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "<span id=\"sec_laplacian_matrix\"></span>\n",
      "The Laplacian matrix"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "After the adjacency matrix, the next most important element of spectral network theory is the *Laplacian matrix*. The Laplacian combines two related features of a network's topology &ndash; its adjacency and its degree distribution &ndash; into a single matrix that can then be used to compute all sorts of other useful properties. You can think of the Laplacian as a \"generalised adjacency matrix\"."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Origins of the Laplacian: diffusion"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "To understand the Laplacian matrix, we need to understand where it comes from.\n",
      "\n",
      "[Pierre-Simon de Laplace](https://en.wikipedia.org/wiki/Pierre-Simon_Laplace) was a French mathematician working in the late 18th and early 19th centuries. He made semial contributions to a number of fields, notably astronomy and mathematical physics, but he is perhaps best known for his contributions to calculus and its applications to fluid mechanics. He defined the Laplacian operator that forms the basis for analysing fluid flow using differential equations.\n",
      "\n",
      "The diffusion of a substance through a gas or liquid is controlled by a very simple process. A simple example is if we have some salt dissolved in a small volume of water, and add this mixture to a larger volume of water. We would expect the salt to diffuse through the larger volume of water over time, making it uniformly salty. The rules that govern this process are surprisingly simple. We divide the larger water volume into small \"packets\" and measure the saltiness of each. Suppose a packet is more salty than all its neighbours, Salt will diffuse out into the neighbours at a rate that is proprtional to the relative difference in saltiness between the two packets. Initially, when we add the salty water, the packets that recive the salty water will be far more salty than their surroundings, and will rapidly lose salt; over time, as the water reaches uniform saltiness, the rate of diffusion will slow.\n",
      "\n",
      "This process can be described mathematically using vector differential equations and the [Laplacian operator](https://en.wikipedia.org/wiki/Laplace_operator).\n",
      "\n",
      "..."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "<span id=\"sec_diffusion\"></span>\n",
      "Diffusion on a network"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The physical diffusion process is typically assumed to occur in a simple vessel like a tank of water. We can however consider diffusion over a *network* as well. To use an analogy, ideas diffuse through the scientific community between colleagues, with the rate of diffusion being controlled by the novelty of the idea: when you first hear it, it's exciting and you race to tell others, but it loses its potency as it spreads further and gradually becomes familiar. The idea follows the edges of the scientists' social network, and so is constrained only to spread between neighbours defined by network adjacency: those people who know each other. (Like all analogies, this one rapidly gets rather tenuous, but hopefully you get the idea.)\n",
      "\n",
      "We can set up a diffusion process simply. Suppose we have a network within which we associate a \"concentration\" $\\psi$ of some substance to each node. At each timestep, we look at all neighbouring pairs of nodes $i$ and $j$ and compare the values $\\psi_i$ and $\\psi_j$ at each. We then \"diffuse\" between them as some rate $r$ from higher to lower concentrations at a rate $r(\\psi_j - \\psi_i)$. If the network's adjacency matrix is $\\mathbf{A}$, then the rate of change of concentration at $i$ is given by:\n",
      "\n",
      "$$\n",
      "    \\frac{d\\psi_i}{dt} = r \\sum_j A_{ij} (\\psi_j - \\psi_i)\n",
      "$$\n",
      "\n",
      "where $j$ ranges over all nodes in the network. If $i$ is less concetrated that $j$, then $\\psi_j - \\psi_i$ is positive and $\\psi_i$ increases; otherwise, the reverse happens; if the two concentrations are exactly equal, no flow occurs. If the two nodes are not adjacent, or if $i$ and $j$ are the same node, $A_{ij}$ is zero and so, again, no flow occurs.\n",
      "\n",
      "We can re-arrange this equation slightly as:\n",
      "\n",
      "\\begin{align*}\n",
      "    \\frac{d\\psi_i}{dt} &= r \\sum_j A_{ij} \\psi_j - r \\sum_j A_{ij} \\psi_i \\\\\n",
      "                       &= r \\sum_j A_{ij} \\psi_j - r \\psi_i \\sum_j A_{ij} \\\\\n",
      "                       &= r \\sum_j A_{ij} \\psi_j - r \\psi_i k_i \\\\\n",
      "                       &= r \\sum_j (A_{ij} - \\delta_{ij} k_i) \\psi_j\n",
      "\\end{align*}\n",
      "\n",
      "where $k_i$ is the degree of $i$ and $\\delta_{ij}$ is the *Kronecker delta* function that is 1 if $i = j$ and 0 otherwise. (The last step looks a little mysterious &ndash; where has $\\psi_i$ gone? &ndash; but because $j$ ranges over all nodes the first term in the sum ($A_{ij}$) picks out adjacent $j$ nodes while the second term ($\\delta_{ij} k_i$) picks out the degree of $i$ multiplied by $\\psi_i$ (since $\\psi_i = \\psi_j$ when $i = j$) as required in the step above.)\n",
      "\n",
      "This expression is actually a family of expressions for all $i$ and $j$, which suggests that we can write it in matrix form, and indeed this is the case. Looking at the last step, we can clearly using the adjacency matrix $\\mathbf{A}$ for the first term; for the second, the Kronecker delta picks out values where $i = j$, which in matrix terms means along the main diagonal. We can therefore re-express the above expression as:\n",
      "\n",
      "$$\n",
      "    \\frac{d\\mathbf{\\psi}}{dt} = r(\\mathbf{A} - \\mathbf{D}) \\mathbf{\\psi}\n",
      "$$\n",
      "\n",
      "where $\\mathbf{\\psi}$ is an $i$-dimensional column vector holding the concentrations at each node $i$; $\\mathbf{A}$ is the adjacemcy matrix; and $\\mathbf{D}$ is a *degree matrix* that has $k_i$ at $D_{ii}$ and zero elsewhere:\n",
      "\n",
      "$$\n",
      "    \\mathbf{D} = \\begin{pmatrix}\n",
      "                     k_1    & 0      & 0      & \\ldots & 0      \\\\\n",
      "                     0      & k_2    & 0      & \\ldots & 0      \\\\\n",
      "                     0      & 0      & k_3    & \\ldots & 0      \\\\\n",
      "                     \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
      "                     0      & 0      & 0      & 0      & k_N\n",
      "                 \\end{pmatrix}\n",
      "$$\n",
      "\n",
      "The [Laplacian matrix](https://en.wikipedia.org/wiki/Laplacian_matrix) is conventionally defined as:\n",
      "\n",
      "$$\n",
      "    \\mathbf{L} = \\mathbf{D} - \\mathbf{A}\n",
      "$$\n",
      "\n",
      "giving an expression for the diffusion:\n",
      "\n",
      "$$\n",
      "    \\frac{d\\mathbf{\\psi}}{dt} + r \\mathbf{L} \\mathbf{\\psi} = 0\n",
      "$$\n"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Structure of the Laplacian"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "What does the Laplacian actually consist of? The adjacency matrix $\\mathbf{A}$ has $A_{ij} = 1$ if $i$ and $j$ are adjacent, and therefore has 0 on the main diagonal. The matrix $\\mathbf{D}$ has $D_{ii} = k_i$ down the main diagonal and 0 elsewhere. The two matrices have the same dimensions, $N \\times N$. Therefore $\\mathbf{L} = \\mathbf{D} - \\mathbf{A}$ has $L_{ii} = k_i$, the node degrees on the main diagonal; $L_{ij} = -1$ where $i$ and $j$ are adjacent; and $L_{ij} = 0$ elsewhere.\n",
      "\n",
      "..."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "er = nx.erdos_renyi_graph(20, 0.01)\n",
      "print np.matrix(nx.laplacian_matrix(er))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[[ <20x20 sparse matrix of type '<type 'numpy.int64'>'\n",
        "\twith 11 stored elements in Compressed Sparse Row format>]]\n"
       ]
      }
     ],
     "prompt_number": 48
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "The Laplacian, components, and connectivity"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The [components of a network](concepts.ipynb#sec_components) are its sub-networks that are connected within themselves but not between themselves.\n",
      "\n",
      "...\n"
     ]
    }
   ],
   "metadata": {}
  }
 ]
}