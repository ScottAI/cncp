{
 "metadata": {
  "name": "",
  "signature": "sha256:c5f1d003ab5779091065456ecc800147f71749e5901bfae314b7700842fa7d4b"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Working with very large networks"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "As you may have already discovered, computations on networks can be very computationally expensive. A large network with lots of edges has lots of paths that may need to be explored to compute metrics, or a lot of information to propagate when exploring dynamics. Although modern computers are fact, they can still be challenged by network science.\n",
      "\n",
      "There are several ways to address these computational needs. We can make use of multiple local cores on the increasingly common multicore desktops (and even laptops). We can use many computers, either within a local area or in the cloud. Or we can use dedicated servers running compute infrastructures like [Hadoop](https://hadoop.apache.org/) or [Spark](https://spark.apache.org/). \n",
      "\n",
      "IPython has some very useable parallel programming features built-in. Beyond these, we can connect Python to the cloud or to a distributed compute engine quite easily, allowing us to make use of Python's libraries in a high-performance environment.\n",
      "\n",
      "The tooling for parallel computing isn't really the issue, though: more important are the thought processes that go into taking a program written to compute over small-ish networks and converting it to a parallel program capable of tackling large-ish (or even insanely large) networks. In this chapter we'll first look at the design aspects of parallel programming, and then use them to take some of the computations from earlier chapters and run them on different computing infrastructures. "
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Thinking parallel"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Parallel programming is a huge area that's attracted enormous attention over the years: I actually did my PhD in programming environments for scaleable parallelism in the mid-1990's. It is driven by a simple need for speed. Despite the improvement in computing performance that's been delivered steadily by Moore's Law, scientists are always several steps ahead in looking at problems that require more compute power than can reasonably be delivered by a single machine. This doesn't stop simgle machines overtaking parallel machines regularly, and I can still remember the shock of seeing the carefully-crafted image processing algorithms we'd spent months parallelising on a [Transputer](https://en.wikipedia.org/wiki/Transputer) array (which had been cutting-edge hardware less than a year earlier) being blown out of the water by a conventional sequential algorithm running on a new [IBM RS/6000](https://en.wikipedia.org/?title=RS/6000). But in the main the leading edge of simulation has often needed parallelism in order to get the performance it needs.\n",
      "\n",
      "At the risk of massive over-simplification, parallel algorithms can be divided into two categories:\n",
      "\n",
      "1. The trivial; and\n",
      "1. The impossible\n",
      "\n",
      "What this means is that some patterns of parallel computation are now well-understood and can be implemented very effectively, so algorithms structured in this way can run very efficiently. Algorithms that *can't* be structured in one of these ways, and so need the programmer to design bespoke and complex computational and communications structures, are typically extremely hard if not impossible to design, debug, optimise, and therefore run with any degree of confidence or performance. The confidence part matters: without a level of confidence in the programming, one can have little confidence in the predictions being made, and run the risk that the whole analysis will be flawed and misleading. It's therefore far preferable to stick to the well-understood patterns wherever possible.\n",
      "\n",
      "The main advances in the design of parallel programs have come about when a style of computation has become understood &ndash; or to put it another way, when some algorithms have transitioned from the \"impossible\" category to the \"trivial\" category."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Why parallel programming is different"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Most algorithms are designed to run sequentially. When designed around arrays, for example, they might start with the first element and process it, then move to the second, and so forth.\n",
      "\n",
      "What we're instructing the machine to do here is to process the data structure (an array in this case) one element at a time. This of course is what machines do at their lowest level, accessing data from memory and applying instructions to it. It's an easy, straightforward, and above all easily-comprehensible way to describe an algorithm that's taught to all programmers from when they first start programming. But it's not the only way to express computation.\n",
      "\n",
      "This \"reductionist\" style of programming is sometimes referred to as the *von Neumann style*. [John von Neumann](https://en.wikipedia.org/wiki/John_von_Neumann) was another giant in the history of mathematics and computing, and his design for computers &ndash; the *von Neumann architecture* consisting a single central processing unit connected to memory, disc, and other peripherals &ndash; has influenced all the computers ever built. Although modern machines don't strictly follow von Neumann's design in at the hardware level, they typically take great pains to *behave* as though they do, and it's the mental model model programmers typically have of the way their computer is organised. The problem is that this forces algorithms to run sequentially because that'd how they're explicitly written. This limitation has been called the *von Neumann bottleneck* &ndash; although that's unfair to von Neumann, who intended his architecture as a reference model rather than a design approach that would persist for sixty-odd years.\n",
      "\n",
      "There is another style of programming, however, that focuses on describing how a program deals with data structures as a whole. Rather than write loops that traverse arrays one element at a time, for example, this style provides functions that (for example) apply the *same* operation to *all* elements of the array in one go, or to reduce all the elements through repeated application of some binary operator. The programmer writes her program to manipulate the entire array in one go, using the \"bulk\" operators. Internally the bulk operations might be written as loops, as in the von Neumann style, but &ndash; critically &ndash; they might be written to work in parallel. Algorithms written in this style therefore aren't *inherently* sequential (although they might be realised that way): they have the *opportunity* for parallelism.\n",
      "\n",
      "(This style is often associated with functional programming, but that's misleading. Functional programming is concerned with lack of side effects, amongst other concerns, which do make the \"bulk\" style easier to work with, but aren't necessary for it. It's perfectly possibly apply bulk operations in traditional or object-oriented languages with side effects. There are plenty of reasons to adopt functional languages and a functional style, but parallelism isn't really a very good one: you can get the same benefits in languages that require less cognitive effort and provide more standard libraries.)\n",
      "\n",
      "Building an algorithm that can be parallelised is therefore different to writing sequential algorithms. Often it's easier if one thinks at a larger scale &ndash; in terms of entire vectors or arrays, rather than their elements &ndash; and code-up the algorithm using appropriate bulk operators. Many of these operators can, it turns out, be effectively parallelised, and once that's happened, *all* programs that use them can gain from parallelism: the programs essentially transition from the \"impossible\" the the \"trivial\" category through clever coding of a small set of operators that work for an entire class of programs, rather than through clever coding of a single program. This is a much more productive use of programmer talent and time."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "What can (and can't) be parallelised"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "There are two catches, however. Firstly, not all algorithms can actually be phrased using bulk operations. Secondly, parallelisation doesn't happen magically or without cost."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Ideas for effective parallel programming"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Think bulk\n",
      "\n",
      "Minimise dependencies\n",
      "\n",
      "Keep the workflow simple\n",
      "\n",
      "Work and debug gradually"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Native IPython parallelism"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "IPython has some useful parallel programming constructs built-in. They're so useful and so integrated into the notebook interface that they make a perfect place to start &ndash; and indeed to stop, for the many applications that won't need the sophistication of a dedicated compute engine."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from IPython.parallel import Client"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "IPython's model is based around a master-slave architecture. The master is the interactive IPython notebook, often running in a web browser; the slaves are other processes running IPython and waiting for instructions from the master. The master creates data and work and sends it to the slaves for computation; the slaves run the code they're sent and return the results to the master for further processing or display. The clever part of the process is that it's done *asynchronously*, meaning that the master stays responsive and can get on with other things while the clients are working."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "c = Client(profile = 'local')\n",
      "cluster = c[:]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import networkx\n",
      "\n",
      "with cluster.sync_imports():\n",
      "    import networkx"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "importing networkx on engine(s)\n"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "g = networkx.erdos_renyi_graph(100, 0.05)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "cluster['nodes'] = g"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%px d = networkx.diameter(nodes)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "CompositeError",
       "evalue": "one or more exceptions from call to method: execute\n[0:execute]: NetworkXError: Graph not connected: infinite path length\n[1:execute]: NetworkXError: Graph not connected: infinite path length",
       "output_type": "pyerr",
       "traceback": [
        "[0:execute]: ",
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
        "\u001b[0;31mNetworkXError\u001b[0m                             Traceback (most recent call last)\u001b[0;32m<ipython-input-1-d2a2de56e009>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m",
        "\u001b[0;32m----> 1\u001b[0;31m \u001b[0md\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnetworkx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiameter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m",
        "\u001b[0m\u001b[0;32m/Users/sd/research/writing/social-sir/cncp/lib/python2.7/site-packages/networkx/algorithms/distance_measures.pyc\u001b[0m in \u001b[0;36mdiameter\u001b[0;34m(G, e)\u001b[0m",
        "\u001b[1;32m     94\u001b[0m     \"\"\"",
        "\u001b[1;32m     95\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0me\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m",
        "\u001b[0;32m---> 96\u001b[0;31m         \u001b[0me\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meccentricity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mG\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m",
        "\u001b[0m\u001b[1;32m     97\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m",
        "\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m",
        "\u001b[0;32m/Users/sd/research/writing/social-sir/cncp/lib/python2.7/site-packages/networkx/algorithms/distance_measures.pyc\u001b[0m in \u001b[0;36meccentricity\u001b[0;34m(G, v, sp)\u001b[0m",
        "\u001b[1;32m     61\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mL\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m",
        "\u001b[1;32m     62\u001b[0m             \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Graph not connected: infinite path length\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m",
        "\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mnetworkx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNetworkXError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m",
        "\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m",
        "\u001b[1;32m     65\u001b[0m         \u001b[0me\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlength\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m",
        "\u001b[0;31mNetworkXError\u001b[0m: Graph not connected: infinite path length",
        "",
        "[1:execute]: ",
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
        "\u001b[0;31mNetworkXError\u001b[0m                             Traceback (most recent call last)\u001b[0;32m<ipython-input-1-d2a2de56e009>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m",
        "\u001b[0;32m----> 1\u001b[0;31m \u001b[0md\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnetworkx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiameter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m",
        "\u001b[0m\u001b[0;32m/Users/sd/research/writing/social-sir/cncp/lib/python2.7/site-packages/networkx/algorithms/distance_measures.pyc\u001b[0m in \u001b[0;36mdiameter\u001b[0;34m(G, e)\u001b[0m",
        "\u001b[1;32m     94\u001b[0m     \"\"\"",
        "\u001b[1;32m     95\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0me\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m",
        "\u001b[0;32m---> 96\u001b[0;31m         \u001b[0me\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meccentricity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mG\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m",
        "\u001b[0m\u001b[1;32m     97\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m",
        "\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m",
        "\u001b[0;32m/Users/sd/research/writing/social-sir/cncp/lib/python2.7/site-packages/networkx/algorithms/distance_measures.pyc\u001b[0m in \u001b[0;36meccentricity\u001b[0;34m(G, v, sp)\u001b[0m",
        "\u001b[1;32m     61\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mL\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m",
        "\u001b[1;32m     62\u001b[0m             \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Graph not connected: infinite path length\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m",
        "\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mnetworkx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNetworkXError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m",
        "\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m",
        "\u001b[1;32m     65\u001b[0m         \u001b[0me\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlength\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m",
        "\u001b[0;31mNetworkXError\u001b[0m: Graph not connected: infinite path length",
        ""
       ]
      }
     ],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "cs = cluster.gather('cs')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "cs.wait()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 10
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Setting up a remote cluster"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Running on a local workstation soon become suntenable when trying to perform research-grade simulations: the sizes of the networks and the number of repetitions needed simply overwhelms a single machine, and even if you upgrade you'll soon be wanting more power. There are two solutions:\n",
      "\n",
      "1. Use a local computing cluster\n",
      "1. Use a cloud computing provider\n",
      "\n",
      "In this section we'll deal with running on a cluster."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Cluster computing"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Until very recently, cluster computing was the most common form of high-performance computing. While they're rapidly being superceded by cloud services, clusters remain a useful way to get more performance cheaply from local resources.\n",
      "\n",
      "Clusters come in two flavours: *dedicated* clusters built for the purpose, and *workstation* clusters that make use of \"spare\" computing power available from machines on people's desk or labs when they're not using them. Clusters come in a bewildering range of flavours. The most common use Linux, with [Rocks](http://www.rocksclusters.org) being very popular. The concepts are all similar, though, whatever flavour you have available.\n",
      "\n",
      "..."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Planning a remote cluster"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The nice thing about using a system like IPython is that is isolates you, as far as possible, from the underlying mechaics of cluster-building. (This won't be true later when we talk about [cloud computing](#sec_cloud_computing), where you typically end up committed to a particular service provider very early on. This is yet another attraction of using clusters.) From IPython's perspective, you simply attach to a cluster controller (an instance of `ipcontroller` running somewhere) and then fire compute jobs at it. What could possibly go wrong?\n",
      "\n",
      "Quite a lot, as it turns out. There are essentially two problems we need to deal with.\n",
      "\n",
      "Firstly, we have to set up the controller and engines so that they all talk to each other within the cluster. Typically a cluster will be sitting on its own local-area network (LAN), where all the machines can see each other and communicate directly. Sometimes this isn't the case, but for the moment we'll assume it is, since it's true for all but the most paranoid of installations.\n",
      "\n",
      "Secondly, we have to let the IPython client talk to the cluster controller. This may be equally easy, but is more often more difficult, since client and cluster may live on different networks. Even if they do happen to be directly visible to each other, though, you might decide to go on the road outside your LAN &ndash; to work from home, for example &ndash; and have to access the cluster across a firewall. Since this is a more common case, we'll assume that this is the situation you're facing: a cluster that's local to itself, but one the other side of a firewall from your client machine at least some of the time. In most cases this means accessing the cluster over an `ssh` tunnel."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "`ssh` tunnels"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "If you're familiar with `ssh`, it will be sufficient to tell you to generate a keypair, install them on the cluster machines, and use `ssh-agent` when setting up the cluster &ndash; and you can now skip the rest of this section.\n",
      "\n",
      "If, on the other hand, that made no sense to you, read on.\n",
      "\n",
      "[EXPLAIN ssh KEYPAIRS, TUNNELS, ssh-agent]"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Configuring a remote cluster"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Getting IPython to use a remote cluster is basically just like getting it to use a local one, with the sole difference that we need to get the cluster machines talking, tell the client where the cluster controller is, and make sure everything is accessible. Fortunately this is all fairly easy. It's easiest if we start at the cluster side and then progress to the client.\n",
      "\n",
      "All the things we need to set are set from an IPython profile, so we'll do that first. On the cluster head, we'll first create an IPython profile:"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<tt>\n",
      "ipython profile create <i>cluster</i> --parallel\n",
      "</tt>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "where *cluster* is the name of the profile, which seems fairly sensible in general but you might want to name your profile after the name of the cluster it describes. (In St Andrews I use the *blob* cluster, so my profile is called *blob*.) As before, this creates all the configuration files we need. We do, however, need to edit things slightly.\n",
      "\n",
      "We need to edit the file `.ipython/profile_cluster/ipcontroller_config.py` (assuming our profile was called *cluster*). Opening this file in an editor will reveal a load of commented-out Python code that sets various parameters that control the behaviour of the controller. The vast majority of these can be left as they are for the vast majority of installations, but we need to make the following modifications by uncommenting variables and setting their values:\n",
      "\n",
      "1. Change `c.IPControllerApp.ssh_server` to the name of the machine that hosts the `ssh` server we'll use to access the cluster from the client. Typically this is the public name of the cluster head *as seen from outside*.\n",
      "2. Change `c.IPControllerApp.reuse_files` to `True`. We'll explain this below. \n",
      "3. Change `c.HubFactory.ip` to  `u'*'`. This will cause the controller to listen on all network interfaces for engines.\n",
      "4. Change `c.IPControllerApp.location` to the name of the cluster head *as seen by engines*.\n",
      "\n",
      "What do we mean by the names of machines as seen by the client and by engines? In many network set-ups the worker nodes in a cluster are on their own network, and may know about only a small set of machines &ndash; so you couldn't access machines over the internet from a worker. The cluster head node, by contrastm is typically accessible more or less publically, and so will have a public name. (In my case the worker nodes think the cluster head is called `blob-cs.local` while the outside world think it's called `blob.cs.st-andrews.ac.uk`, the machine I can `ssh` into. So these two machines names would go into `c.IPControllerApp.location` and `c.IPControllerApp.ssh_server` respectively.)\n",
      "\n",
      "That's it as far as the cluster is concerned. We now need to configure the client-side. On the machine running IPython we create a profile again, which may or may not have the same name as the cluster-side one: it makes sense for them to have the same name most of the time, and it will probably cause endless confusion if you don't, but you don't *have* to use the same name. The edits to this profile are simpler: we just need to tell the client where the cluster head is:\n",
      "\n",
      "1. Change `c.IPControllerApp.ssh_server` to the public name of the cluster head as seen from outside.\n",
      "\n",
      "(This is the same change as step 1 on the cluster side.) That's it! For most cluster set-ups where the workers and head nodes share a filing system, that's all we have to do."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Starting the cluster"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We're now ready to start the cluster. When we did this earlier we simply started everything from the IPython notebook. We can't do this for a remote cluster, as a typical workflow is to start the cluster and leave it running, connecting from outside as required. The cluster topology is also more complicated than a local cluster, in that the controller is accessed by multiple engines, each living on a different machine: this is after all the point, to get access to lots of machines' computing power simultaneously.\n",
      "\n",
      "The mechanism is very simple. We first start a controller on the head node, then start engines on the worker nodes, and finally ship some security information to the client to let it connect.\n",
      "\n",
      "Starting the controller is simple, on the head node just run:"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<tt>\n",
      "(ipcontroller --profile=cluster &)\n",
      "</tt>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "(Again assuming *cluster* is the name of your profile.) This puts the controller into the background: you'll see some status information appearing in the terminal, that can usually just be used to provide confidence that something is happening.\n",
      "\n",
      "How you start the engines depends on the cluster management software, and can range from having to log-in to all the machines individually to running some more automated process. If we assume we're using Rocks, we can do the following:"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<tt>\n",
      "rocks run host command='(ipengine --profile=cluster &)'\n",
      "</tt>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Much status information will appear, again hopefully just confirming things are working. At this stage we hopefully have a cluster controller running on the head node, connected to engines sitting ready for work on the worker nodes.\n",
      "\n",
      "[ADD SOME SCREENSHOTS OF ALL THIS]\n",
      "\n",
      "[EXPLAIN HOW TO DO THIS MANUALLY]\n",
      "\n",
      "[TROUBLESHOOTING]"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Connecting to the cluster"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Finally we need to connect to the controller from IPython. We need a final step to make the security work. Why security? IPython will not let *anyone* run jobs on a cluster, even if they can log-in to the cluster head. We need to provide the client with a key to allow it access. When the controller starts, it generates a key file (plus some other metadata) and stores it as `.ipython/profile_cluster/security/ipcontroller-client.json`. We copy this file from the cluster head to the client, placing it into the client-side profile directory at the same location. We therefore have two machines &ndash; client and cluster head &ndash; with potentially slightly different IPython profiles but each with a copy of the key file that the controller created. If we open a connection to the cluster, IPython will use the profile description to find the cluster head, log-in using `ssh`, use the key file to authenticate itself to the controller, and create the necessary connection ready to use the engines.\n",
      "\n",
      "Phew!\n",
      "\n",
      "You may be happy to learn that we don't have to go through this process evey time. The reason for this is the slightly mysterious line in the cluster's profile configuration: step 2 above where we did `c.IPControllerApp.reuse_files = True`. If we kill the cluster (or it dies for some reason), when we re-start the controller it will re-use the same key file, which means that authentication from existing clients will still work. However, if you edit the cluster's configuration for any reason, you'll need to remove the key files, let the controller create new ones, and then copy these down to the client. Once the system is set up and known to be working, though, this very seldom happens. "
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "<span id=\"sec_cloud_computing\"></span>\n",
      "Cloud computing"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Summary"
     ]
    }
   ],
   "metadata": {}
  }
 ]
}