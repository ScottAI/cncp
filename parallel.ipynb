{
 "metadata": {
  "name": "",
  "signature": "sha256:c8a01fbaac65cb14624509fd37102225a4f055a20d8f3e88cca1e2c7883b460c"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "<span id=\"chap_parallel\"></span>\n",
      "Working with very large networks"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "So far so good: we've looked at a simple kind of complex network, and a simple kind of complex process. We've shown how we can analyse them analytically and simulate them, using the interactive capabilities of IPython to control computations, gather the results, produce graphs, and then make the whole lot available on the web.\n",
      "\n",
      "If you've taken the opportunity to look at any of the research papers mentioned in the text so far, however, you may have noticed something that makes you slightly uncomfortable. Simulations need to be run several times for different parameter values to even-out the stochastic nature of the network/process interactions. Complex network effects often only work on large networks: in theory as $N \\rightarrow \\infty$, in practice with \"lots\" of nodes and edges. Both these factors combine to generate a *lot* of computation: 10000 simulation repetitions across a space of two parameters over networks of 100000 nodes would not be especially unusual. Performing a simulation like tis on a typical desktop workstation or laptop is clearly not going to work.\n",
      "\n",
      "This has been a persistent story in computer science, of course, and it's usually tackled by appealing to [Moore's law](https://en.wikipedia.org/wiki/Moore%27s_law), the notion that the amount of computing power available at a given price  doubles roughly every eighteen months. However, the programs we commonly write are sequential, and the Moore's law curve is flattening out for individual processor cores, reducing the speed-up available to sequential programs over time. Extra speed-up comes, not from faster processor cores, but from having more cores available, often on the same processor: but taking advantage of this means *parallel* processing with multiple\"threads\" of activity happening simultaneously.\n",
      "\n",
      "To do research-grade simulation of complex networks and complex processes, then, requires that we tackle high-performance parallel computing. While the general techniques of parallel programming are quite arcane, there are specific techniques that are easy to use both conceptually and (to some extent) technically &ndash; and fortunately much of network science simulation fits into these special cases. \n",
      "\n",
      "In this chapter we explore the approaches we need to work with large networks. We start with the concepts of parallel systems as far as we need to understand them for our purposes. We'll then talk about using IPython's parallel tools, which turn out to be well-suited to the sorts of computations we want to perform, and which mean we can utilise high-performance computing from within the same interactive environment we've been using up to now. We'll look at two separate approaches to getting parallel processing: clusters (both local and remote), and cloud computing. In between we'll re-visit the issues of reproducibility that we explored [earlier](simulate.ipynb#sec_repeatability_small), since these are becoming increasingly important to conducting science at scale. "
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "<span id=\"sec_thinking_parallel\"></span>\n",
      "Thinking parallel"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "At its simplest, parallel computing involves writing programs that can perform many tasks simultaneously, making use of more than one processor core to compute with. It surprises some people that this isn't true of all programs, but a typical program is *sequential* and executes a single \"thread\" of instructions. If a computer looks like it's doing several things simultaneously, it's just because it's splitting its time between multiple (sequential) tasks very quickly. *Real* parallelism, which is the only way to leverage the power of more than one core for solvoing a single task or problem, requires that we think differently about programming.\n",
      "\n",
      "[MORE]\n",
      "\n",
      "At the risk of massive over-simplification, parallel algorithms can be divided into two broad categories:\n",
      "\n",
      "1. The trivial; and\n",
      "1. The impossible\n",
      "\n",
      "What this means is that some patterns of parallel computation are now well-understood and can be implemented very effectively, so algorithms structured in this way can run very efficiently. Algorithms that *can't* be structured in one of these ways need a programmer to design bespoke and complex computational and communications structures. Such programs are typically extremely hard if not impossible to design, code, debug, and optimise &ndash; and therefore often cannot be run with any degree of confidence. The confidence part matters: without a level of confidence in the programming, one can have little confidence in the predictions being made, and run the risk that the whole analysis will be flawed and misleading. It's therefore far preferable to stick to the well-understood patterns wherever possible.\n",
      "\n",
      "The main advances in the design of parallel programs have come about when a style of computation has become understood &ndash; or to put it another way, when some algorithms have transitioned from the \"impossible\" category to the \"trivial\" category."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Why parallel programming is different"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Most algorithms are designed to run sequentially. When designed around arrays, for example, they might start with the first element and process it, then move to the second, and so forth.\n",
      "\n",
      "What we're instructing the machine to do here is to process the data structure (an array in this case) one element at a time. This of course is what machines do at their lowest level, accessing data from memory and applying instructions to it. It's an easy, straightforward, and above all easily-comprehensible way to describe an algorithm that's taught to all programmers from when they first start programming. But it's not the only way to express computation.\n",
      "\n",
      "This \"reductionist\" style of programming is sometimes referred to as the *von Neumann style*. [John von Neumann](https://en.wikipedia.org/wiki/John_von_Neumann) was another giant in the history of mathematics and computing, and his design for computers &ndash; the *von Neumann architecture* consisting a single central processing unit connected to memory, disc, and other peripherals &ndash; has influenced all the computers ever built. Although modern machines don't strictly follow von Neumann's design in at the hardware level, they typically take great pains to *behave* as though they do, and it's the mental model model programmers typically have of the way their computer is organised. The problem is that this forces algorithms to run sequentially because that'd how they're explicitly written. This limitation has been called the *von Neumann bottleneck* &ndash; although that's unfair to von Neumann, who intended his architecture as a reference model rather than a design approach that would persist for sixty-odd years.\n",
      "\n",
      "There is another style of programming, however, that focuses on describing how a program deals with data structures as a whole. Rather than write loops that traverse arrays one element at a time, for example, this style provides functions that (for example) apply the *same* operation to *all* elements of the array in one go, or to reduce all the elements through repeated application of some binary operator. The programmer writes her program to manipulate the entire array in one go, using the \"bulk\" operators. Internally the bulk operations might be written as loops, as in the von Neumann style, but &ndash; critically &ndash; they might be written to work in parallel. Algorithms written in this style therefore aren't *inherently* sequential (although they might be realised that way): they have the *opportunity* for parallelism.\n",
      "\n",
      "(This style is often associated with functional programming, but that's misleading. Functional programming is concerned with lack of side effects, amongst other concerns, which do make the \"bulk\" style easier to work with, but aren't necessary for it. It's perfectly possibly apply bulk operations in traditional or object-oriented languages with side effects. There are plenty of reasons to adopt functional languages and a functional style, but parallelism isn't really a very good one: you can get the same benefits in a sufficiently rich imperative language, and keep access to a better range of libraries.)\n",
      "\n",
      "Building an algorithm that can be parallelised is therefore different to writing a sequential algorithm. Often it's easier if one thinks at a larger scale &ndash; in terms of entire vectors or arrays, rather than their elements &ndash; and code-up the algorithm using appropriate bulk operators. Many of these operators can, it turns out, be effectively parallelised, and once that's happened, *all* programs that use them can gain from parallelism: the programs essentially transition from the \"impossible\" the the \"trivial\" category through clever coding of a small set of operators that work for an entire class of programs, rather than through clever coding of a single program. This is a much more productive use of programmer talent and time."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "What can (and can't) be parallelised"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "There are two catches, however. Firstly, not all algorithms can actually be phrased using bulk operations. Secondly, parallelisation doesn't happen magically or without cost.\n",
      "\n",
      "[INHERENT SEQUENTIALITY]\n",
      "\n",
      "[THE GLOBAL INTERPRETER LOCK]"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "<span id=\"sec_ipython_parallel\"></span>\n",
      "Native IPython parallelism"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "IPython has some useful parallel programming constructs built-in. They're so useful and so integrated into the notebook interface that they make a perfect place to start &ndash; and indeed to stop, for the many applications that won't need the sophistication of a dedicated compute engine."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "The IPython parallel computing architecture"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "There are several ways in which one can set up a parallel computing system. IPython's approach is to provide an architecture within which to tie-together several IPython processes that can be managed *en bloc* to get improved performance. This is complemented by a library that provides portable ways to connect to and control a set of workers.\n",
      "\n",
      "IPython's parallel library is actually very general and provides for several different parallel processing idioms. In the interests of simplicity, in this chapter we'll deal only with those specific approaches that we think are useful for simulating networks. If you need something more flexible, or want to understand parallel computing more generally, there are plenty of material available: a good on-line tutorial is [[GPRK11]](bibliography.ipynb#GPRK11), while [[Ros13]](bibliography.ipynb#Ros13) also covers the material in some depth. Nonetheless we've found that a lot of the available material elides details that are needed in practice when running large, long-lived simulations from an interactive IPython notebook: we'll spell-out how to make this work in this chapter. \n",
      "\n",
      "When we're thinking about the architecture, we need to consider both its physical and logical elements. The physical elements are a set of machines connected by one of more interconnects: I'll use the term \"interconnect\" to refer to communication links between machines, instead of the more usual term \"network\", for reasons that are hopefully obvious! Typically we have a number of machines being used for computing, and we'll refer to these machines as a *cluster*. Within the cluster most of the machines are usually dedicated to providing compute services, and we refer to these as *workers*. There is a single machine that doesn't provide computing but instead manages the compute services providee by the others, and we'll refer to this machine as the *cluster head*. Outside the cluster there will be one or more *client* machines being used by scientists, which may be deaktop or laptop machines, and may be powerful in their own right.\n",
      "\n",
      "The logical elements are a set of processes running on the physical elements. Each client will have one or more IPython processes to manage the interface with the users, and we'll refer to these processes generically as *notebooks* even though they might actually be just scripts. On the cluster head there is a single *controller* that acts as the gateway to the cluster's compute resources. On the cluster workers there are a number of *engines* that connect to the controller. How many engines? There can be as many as you want on each worker, but modern servers are becoming increasingly multicore and typically one would start as many engines as the worker has cores.\n",
      "\n",
      "To recap, the notebook views the cluster through the controller, which is connected to engines running on cluster workers, with as many engines as the cluster workers have cores. The idea is that work generated in the notebook will be executed by the engines, and there are enough engines to soak-up all the computing resources available in the cluster. "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<div class=figure id=fig_ipython_architecture>\n",
      "<div class=figurebody>\n",
      "<img alt=\"IPython parallel computing architecture\" src=\"ipython-parallelism.svg\">\n",
      "<br>\n",
      "<span class=caption>The IPython parallel architecture. Processes map to different machines. Workers may have several engines to exploit multiple processor cores</span>\n",
      "</div>\n",
      "</div>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This is the most general layout for IPython parallel computing. There are some important variants, the most important of which are the *local cluster* that we'll describe next, an the *remote clusters* that we return to [below](#sec_remote_cluster).\n",
      "\n",
      "Suppose you have a single laptop or workstation. Modern machines are increasingly powerful, and even personal machines are becoming multicore, so you can get respectable, if limited, parallel processing even when you don't have access to a cluster. In this case the client and the cluster are *all the same machine* at the physical level, but we still use the *same* logical architecture of processes."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<div class=figure id=fig_ipython_local_cluster>\n",
      "<div class=figurebody>\n",
      "<img alt=\"IPyton parallelism on a single machine\" src=\"ipython-local-parallelism.svg\">\n",
      "<br>\n",
      "<span class=caption>A local cluster. The logical architecture, and therefore the view from software, remains the same as for the general case.</span>\n",
      "</div>\n",
      "</div>"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Local clusters"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "[USING ipcluster TO START RUNNING LOCALLY] "
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Interacting with the local cluster"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from IPython.parallel import Client"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "IPython's model is based around a master-slave architecture. The master is the interactive IPython notebook, often running in a web browser; the slaves are other processes running IPython and waiting for instructions from the master. The master creates data and work and sends it to the slaves for computation; the slaves run the code they're sent and return the results to the master for further processing or display. The clever part of the process is that it's done *asynchronously*, meaning that the master stays responsive and can get on with other things while the clients are working."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "c = Client()\n",
      "cluster = c[:]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import networkx\n",
      "\n",
      "with cluster.sync_imports():\n",
      "    import networkx"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "importing networkx on engine(s)\n"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "g = networkx.erdos_renyi_graph(100, 0.05)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "cluster['nodes'] = g"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%px d = networkx.diameter(nodes)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "cs = cluster.gather('cs')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "cs.wait()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 10
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "<span id=\"sec_remote_cluster\"></span>\n",
      "Setting up a remote cluster"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Running on a local workstation soon become suntenable when trying to perform research-grade simulations: the sizes of the networks and the number of repetitions needed simply overwhelms a single machine, and even if you upgrade you'll soon be wanting more power. There are two solutions:\n",
      "\n",
      "1. Use a local computing cluster\n",
      "1. Use a cloud computing provider\n",
      "\n",
      "In this section we'll deal with running on a cluster."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Cluster computing"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Until very recently, cluster computing was the most common form of high-performance computing. While they're rapidly being superceded by cloud services, clusters remain a useful way to get more performance cheaply from local resources.\n",
      "\n",
      "Clusters come in two flavours: *dedicated* clusters built for the purpose, and *workstation* clusters that make use of \"spare\" computing power available from machines on people's desk or labs when they're not using them. Clusters come in a bewildering range of flavours. The most common use Linux, with [Rocks](http://www.rocksclusters.org) being very popular. The concepts are all similar, though, whatever flavour you have available.\n",
      "\n",
      "[REMOTE CLUSTER ARRANGEMENTS: SHARED FILES, DEDICATED VS NoW]"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Planning a remote cluster"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The nice thing about using a system like IPython is that is isolates you, as far as possible, from the underlying mechaics of cluster-building. (This won't be true later when we talk about [cloud computing](#sec_cloud_computing), where you typically end up committed to a particular service provider very early on. This is yet another attraction of using clusters.) From IPython's perspective, you simply attach to a cluster controller (an instance of `ipcontroller` running somewhere) and then fire compute jobs at it. What could possibly go wrong?\n",
      "\n",
      "Quite a lot, as it turns out. There are essentially two problems we need to deal with.\n",
      "\n",
      "Firstly, we have to set up the controller and engines so that they all talk to each other within the cluster. Typically a cluster will be sitting on its own local-area network (LAN), where all the machines can see each other and communicate directly. Sometimes this isn't the case, but for the moment we'll assume it is, since it's true for all but the most paranoid of installations.\n",
      "\n",
      "Secondly, we have to let the IPython client talk to the cluster controller. This may be equally easy, but is more often more difficult, since client and cluster may live on different networks. Even if they do happen to be directly visible to each other, though, you might decide to go on the road outside your LAN &ndash; to work from home, for example &ndash; and have to access the cluster across a firewall. Since this is a more common case, we'll assume that this is the situation you're facing: a cluster that's local to itself, but one the other side of a firewall from your client machine at least some of the time. In most cases this means accessing the cluster over an `ssh` tunnel."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "`ssh` tunnels"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "If you're familiar with `ssh`, it will be sufficient to tell you to generate a keypair, install them on the cluster machines, and use `ssh-agent` when setting up the cluster &ndash; and you can now skip the rest of this section.\n",
      "\n",
      "If, on the other hand, that made no sense to you, read on.\n",
      "\n",
      "[EXPLAIN ssh KEYPAIRS, TUNNELS, ssh-agent]"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Configuring a remote cluster"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Getting IPython to use a remote cluster is basically just like getting it to use a local one, with the sole difference that we need to get the cluster machines talking, tell the client where the cluster controller is, and make sure everything is accessible. Fortunately this is all fairly easy. It's easiest if we start at the cluster side and then progress to the client.\n",
      "\n",
      "All the things we need to set are set from an IPython profile, so we'll do that first. On the cluster head, we'll first create an IPython profile:"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<tt>\n",
      "ipython profile create <i>cluster</i> --parallel\n",
      "</tt>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "where *cluster* is the name of the profile, which seems fairly sensible in general but you might want to name your profile after the name of the cluster it describes. (In St Andrews I use the *blob* cluster, so my profile is called *blob*.) As before, this creates all the configuration files we need. We do, however, need to edit things slightly.\n",
      "\n",
      "We need to edit the file `.ipython/profile_cluster/ipcontroller_config.py` (assuming our profile was called *cluster*). Opening this file in an editor will reveal a load of commented-out Python code that sets various parameters that control the behaviour of the controller. The vast majority of these can be left as they are for the vast majority of installations, but we need to make the following modifications by uncommenting variables and setting their values:\n",
      "\n",
      "1. Change `c.IPControllerApp.ssh_server` to the name of the machine that hosts the `ssh` server we'll use to access the cluster from the client. Typically this is the public name of the cluster head *as seen from outside*.\n",
      "2. Change `c.IPControllerApp.reuse_files` to `True`. We'll explain this below. \n",
      "3. Change `c.HubFactory.ip` to  `u'*'`. This will cause the controller to listen on all network interfaces for engines.\n",
      "4. Change `c.IPControllerApp.location` to the name of the cluster head *as seen by engines*.\n",
      "\n",
      "What do we mean by the names of machines as seen by the client and by engines? In many network set-ups the worker nodes in a cluster are on their own network, and may know about only a small set of machines &ndash; so you couldn't access machines over the internet from a worker. The cluster head node, by contrastm is typically accessible more or less publically, and so will have a public name. (In my case the worker nodes think the cluster head is called `blob-cs.local` while the outside world think it's called `blob.cs.st-andrews.ac.uk`, the machine I can `ssh` into. So these two machines names would go into `c.IPControllerApp.location` and `c.IPControllerApp.ssh_server` respectively.)\n",
      "\n",
      "That's it as far as the cluster is concerned. We now need to configure the client-side. On the machine running IPython we create a profile again, which may or may not have the same name as the cluster-side one: it makes sense for them to have the same name most of the time, and it will probably cause endless confusion if you don't, but you don't *have* to use the same name. The edits to this profile are simpler: we just need to tell the client where the cluster head is:\n",
      "\n",
      "1. Change `c.IPControllerApp.ssh_server` to the public name of the cluster head as seen from outside.\n",
      "\n",
      "(This is the same change as step 1 on the cluster side.) That's it! For most cluster set-ups where the workers and head nodes share a filing system, that's all we have to do."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Starting the cluster"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We're now ready to start the cluster. When we did this earlier we simply started everything from the IPython notebook. We can't do this for a remote cluster, as a typical workflow is to start the cluster and leave it running, connecting from outside as required. The cluster topology is also more complicated than a local cluster, in that the controller is accessed by multiple engines, each living on a different machine: this is after all the point, to get access to lots of machines' computing power simultaneously.\n",
      "\n",
      "The mechanism is very simple. We first start a controller on the head node, then start engines on the worker nodes, and finally ship some security information to the client to let it connect.\n",
      "\n",
      "Starting the controller is simple, on the head node just run:"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<tt>\n",
      "(ipcontroller --profile=cluster &)\n",
      "</tt>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "(Again assuming *cluster* is the name of your profile.) This puts the controller into the background: you'll see some status information appearing in the terminal, that can usually just be used to provide confidence that something is happening.\n",
      "\n",
      "How you start the engines depends on the cluster management software, and can range from having to log-in to all the machines individually to running some more automated process. If we assume we're using Rocks, we can do the following:"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<tt>\n",
      "rocks run host command='(ipengine --profile=cluster &)'\n",
      "</tt>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Much status information will appear, again hopefully just confirming things are working. At this stage we hopefully have a cluster controller running on the head node, connected to engines sitting ready for work on the worker nodes.\n",
      "\n",
      "[ADD SOME SCREENSHOTS OF ALL THIS]\n",
      "\n",
      "[EXPLAIN HOW TO DO THIS MANUALLY]\n",
      "\n",
      "[TROUBLESHOOTING]"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Connecting to the cluster"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Finally we need to connect to the controller from IPython. We need a final step to make the security work. Why security? IPython will not let *anyone* run jobs on a cluster, even if they can log-in to the cluster head. We need to provide the client with a key to allow it access. When the controller starts, it generates a key file (plus some other metadata) and stores it as `.ipython/profile_cluster/security/ipcontroller-client.json`. We copy this file from the cluster head to the client, placing it into the client-side profile directory at the same location. We therefore have two machines &ndash; client and cluster head &ndash; with potentially slightly different IPython profiles but each with a copy of the key file that the controller created. If we open a connection to the cluster, IPython will use the profile description to find the cluster head, log-in using `ssh`, use the key file to authenticate itself to the controller, and create the necessary connection ready to use the engines.\n",
      "\n",
      "Phew!\n",
      "\n",
      "You may be happy to learn that we don't have to go through this process evey time. The reason for this is the slightly mysterious line in the cluster's profile configuration: step 2 above where we did `c.IPControllerApp.reuse_files = True`. If we kill the cluster (or it dies for some reason), when we re-start the controller it will re-use the same key file, which means that authentication from existing clients will still work. However, if you edit the cluster's configuration for any reason, you'll need to remove the key files, let the controller create new ones, and then copy these down to the client. Once the system is set up and known to be working, though, this very seldom happens. "
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Long-lived clusters"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "As we ramp-up the scale of simulation, executions take more and more time. This is fine if you can leave you computers alone, which might be the case for a desktop machine in the office. But what if, as is increasingly common, your main machine is a laptop, which will be disconnected at the end of a day? What if you work in the field with flaky internet access? In these cases, we don't want disconnecting from the cluster to cause your simultion to end, and we want to be able to reconnect later to retrieve the results.\n",
      "\n",
      "IPython supports this style of \"disconnected\" operation, whereby you can start a computation going, leave it running while you do other things, possibly closing down or dosconnecting the client computer, and come back later when it's done. The key ideas are *asynchronous calls* from the client and a *database backend* for the cluster.\n",
      "\n",
      "[EXPLAIN ASYNCHRONOUS CALLS AND RESULTS]\n",
      "\n",
      "If asynchronous calls save us waiting to results when we don't want to, a databsse backend makes sure the results are there when we want them. Essentially a database backend causes the cluster head to store the results of the computations it's been asked to perform so that they can be retrieved later, without keeping them in memory (which might be problematic for large result sets).\n",
      "\n",
      "[OPTIONS]\n",
      "\n",
      "We again edit `.ipython/profile_cluster/ipcontroller.py` (assuming our profile is called *cluster*) and make a single change:\n",
      "\n",
      "1. Change `c.HubFactory.db_class` to `IPython.parallel.controller.sqlitedb.SQLiteDB`."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Programming for a remote cluster"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The incantations above will hopefully set up a remote cluster that has exactly the same interface as the local cluster we set up earlier. While this is a huge benefit &ndash; we don't have to commit to using a particular set-up when writing our code &ndash; things aren't quite that simple. \n",
      "\n",
      "There are essentially two problems we need to deal with. Firstly, the purpose of using a cluster is to get performance, and we only get maximum performance if all the engines are kept busy computing all the time. Even if we step back a little from this ideal position, we need to keep the engines *as busy as possible doing useful computing* if we're to get the performance boost that a cluster is able to give us. We need to structure code to this end, which means thinking about how to keep engines evenly supplied with work.\n",
      "\n",
      "Secondly, the remote cluster is, well, *remote*, in the sense that there is a network between the client and the cluster head, and potentially between the cluster head and the worker nodes. Modern networks are fast, but modern simulations can be large, and even a fast network can start to collapse under the pressure of moving lots of data around. Sending lots of data can slow things down substantially, especially as an engine that's exchanging data isn't doing useful computation. So our thoughts about code structure need to deal with two related phenomena: keeping engines fed with work, and keeping them doing that work rather than talking over the network. We'll deal with these two questions in the next two sections."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "<span id=\"sec_parallel_work\"></span>\n",
      "Keeping engines supplied with work"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Keeping engines supplied with work comes in two phases. IPython provides half, but the programmer needs to provide the other.\n",
      "\n",
      "IPython's half is provided by the view we take of the cluster. When we first discussed IPython parallelism [above](#sec_ipython_parallel) we used both *direct* and *load-balanced* views. A direct view lets the programmer send particular jobs to particular engines in the cluster, which is useful if you want each engine to do something different. However, often we want the engines doing the *same* thing but over different data, and this is where a load-balanced view comes in. A load-balanced view takes a set of jobs and allocates them to engines *as they become free*. Suppose we do the following: "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "rc = view.map(f, range(10000000))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "What does this do? If `view` is a direct view, then it tries to run function `f` in parallel for each of the 1000000 values in the range we're applying it to. If we have a cluster with 1000000 engines, that's fine; if not, we can't do this computation.\n",
      "\n",
      "But what if `view` is load-balancing? Suppose we have a reasonably-sized cluster with 128 engines. The load-balanced view will take  and apply it in parallel to the first 128 elements of the range, computing `f(0)`, `f(1)`, and so on up to `f(127)`. These calculations might take radically different amounts of time. When an engine finishes and returns a result, the view gives it the next calculation &ndash; `f(128)` in this case &ndash; to do, and similarly as other engines complete. This all happens transparently of the programmer, so as far as you're concerned you map `f` over the list and get speed-up from parallel execution.\n",
      "\n",
      "There is an implication here, though. If an engine finishes its job, it can only keep busy &ndash; keep contributing useful work &ndash; if it can receive another job. The load-banaced view handles the mechanics of this, but the programmer has to supply the jobs. In some cases jobs all take roughly the same time; in others jobs make radically different computational demands. The ideal is where every engine is kept 100% busy and they all finish their final job at roughly the same time; in reality, we can keep the engines busy until the last jobs ahave been submitted to an engine, and work will trail off as these jobs are finished and no new ones are allocated.\n",
      "\n",
      "From a programming perspective, this suggests structuring programs to have a large number of small jobs: a large number so there is always work available, and small to allow more efficient scheduling. One might, for example, decide to run network simulations where a job is a single run &ndash; a network with a process to run over it &ndash; and perform hundreds (or thousands) or repetitions of each set of parameters being tested.\n",
      "\n",
      "While this is good in principle from this perspective, in practice it has some disadvantages that we'll now consider."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "<span id=\"sec_parallel_chunking\"></span>\n",
      "Locality and chunking"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Suppose we have the following scenario:\n",
      "\n",
      "* We want to simulate a process on a network under two parameters (say $a$ and $b$)\n",
      "* We want to test $a$ and $b$ over a range of ten data points each, so over all pairs of $(a_0, b_), (a_0, b_1), \\ldots , (a_0, b_9), (a_1, b_0), \\ldots , (a_9, b_9)$\n",
      "* Because the process is stochastic, we decide to perform 1000 repetitions of the experiment at each parameter pair, generating a different network for each\n",
      "* To make things reliable, we'll use a network of 100000 nodes for each test\n",
      "\n",
      "Using what we learned above, we could proceed as follows. We define a function for the process dynamics for each pair of parameters. Then, for each pair of parameters, we build a list consisting of 10000 instances of 100000-node networks with the appropriate degree distribution and edges. Then we map the process function across the networks using a load-balanced view. Looking at the numbers, we end up with 100 simulation functions, each mapped across a list of 10000 elements, each of which is a 100000-node network: $10^6$ compute jobs.\n",
      "\n",
      "Does this meet the criteria we set? Well, yes: lots of jobs, each of which, while not small, is about as small as we can make it under the circumstances (a simulation across a single network). So is this a good solution to the problem that we could code-up?\n",
      "\n",
      "No.\n",
      "\n",
      "Why not? Basically for three reasons:\n",
      "\n",
      "1. the memory of the client;\n",
      "1. the volume of traffic over the network; and\n",
      "1. the management overhead.\n",
      "\n",
      "Let's deal with the problems first, starting with the client's memory, and then move to some possible solutions. To run the simulations for a given pair of parameters, we have to build 10000 instances of a 100000-node network. That's going to use a considerable amount of memory, since it's $10^9$ nodes with their associated edges. A slightly different programming approach, and we'd store all the networks for *all* the parameter combinations: $10^{11}$ nodes, with edges. That's a *lot* of storage*\n",
      "\n",
      "Suppose our client machine can cope: OK, we now have to spin-up the simulation jobs, which means passing the network data over the network to the controller, and then to an engine. The controller has to acquire and store the data, and then retain it until it can be passed to an engine. Both of these require communicating a 100000-node network between two machines. That's a *lot* of data movement.\n",
      "\n",
      "While this is happening the client, controller, and engines all have to maintain and exchange information in the background to keep things working. We have to keep the work flowing, possibly monitoring when jobs complete and sending a new job to the now-free engine. Finally, we have to return whatever metrics we derive at the end of the simulation back to the client for processing.\n",
      "\n",
      "We can argue about whether this partcular scenario would work on a particular hardware set-up, and the notion of something being \"too big\" will change over time, but I think it's safe to say that we have too much of a good thing here: too many jobs, too much network traffic, too much overhead, too much memory being used in one place. Each of these \"too much\"es is a recipe for a failure that stops things working and loses data.\n",
      "\n",
      "But didn't we do everything right? Yes and no. Conceptually we did indeed do the right thing. We split up our initial problem into a load of independent computations that could be performed in parallel and then aggregated. But we then simply implemented that solution na&iuml;vely, without considering the reality of the situation. We were on the right track, however: having the right conceptual framework is half-way to a solution, what we need to do is realise this framework in a more realistic manner.\n",
      "\n",
      "There are two techniques we can use to improve our implementation. Firstly, we can think about *where computation occurs*, and specifically about where we build the networks we're going to use: do they have to be passed between machines? Secondly, we can \"chunk\" jobs together and schedule a smaller number of larger jobs: still enough to keep the engines busy, but not enough to cause problems.\n",
      "\n",
      "Let's use these two techniques in our scenario. Conceptually we have a simulation function and a network it runs over. We could have the simulation function build the network itself and then work on it. That sounds equivalent, and indeed it is, at on level: that's really the point. But implementationally, in the first case we created functions and networks at the client and then communicated them to an engine *via* the controller; in the second case we created functions at the client and communicated them, and they then built their network *at the engine*: no communication needed. We've immediately reduced the client's memory requirement and the amount of data communicated &ndash; and, incidentally, speeded things up by allowing the creation of the different networks to happen in parallel at the engines that will then use them.\n",
      "\n",
      "We've still got a lot of jobs. Currently our simulation function performs a single simulation. We could re-code it to perform all 10000 repetitions for a given pair of parameters, building a new random network locally for each repetition. Now instead of $10^6$ jobs (10000 repetitions each of 100 parameter pairs), we have 100 jobs, the number of parameter pairs &ndash; each of which performs 10000 repetitions. Moreover, since the reason we're doing the repetitions is probably to compute average values for the various metrics, we could do *that* computation at the engine and just ship back the average values, rather than the values for each repetition (since we don't care about them). Again, we've reduced communications *and* speeded things up by performing parallel calculations, *and* saved computation at the client since the code returns averages not aw data needing further processing. \n",
      "\n",
      "But we need to pause a moment: 100 jobs does not sound like very many. If we had a cluster with 128 engines &ndash; not unreasonable at the time of writing &ndash; then we only have enough work for 100 of them: we leave around 20% of the engines idle. On the other hand, if we only have 64 engines, we can keep them all busy for longer. If we had 128 engines, maybe we'd want to build smaller chunks &ndash; two jobs doing 5000 repetitions each for each parameter pair, perhaps? &ndash; to give ourselves 200 jobs to spread across the available machines. Alternatively we could increase the number of parameters, maybe explore $a$ and $b$ over 20 points each, yielding 400 jobs each of 10000 repetitions. (If that sounds a bit gratuitous, in reality we often let problems expand to fill the computational power available: a sort of computational version of [Parkinson's law](https://en.wikipedia.org/wiki/Parkinson%27s_law).)\n",
      "\n",
      "Let's recapitulate the journey we've just made. We started with a scientific problem to solve. We build a conceptual model that divided-up the problem into alots of independent jobs to solve independently and then combine together. We then tweaked this model to make it realistic, taking account of the limitations of real computers and their interconnects, and arrived at a model that is tailored to fit actual computational environment we have available. We can now code this up and run it. It's important to note that the steps we've taken haven't really tied us down too much: we'd need to re-code the simulation slightly and provide appropriate values for the number of repetitions and the like, but fundamentally the final code is very like our initial formulation. Later we'll see how to do this in practice.  "
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "<span id=\"sec_cloud_computing\"></span>\n",
      "Cloud computing"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "<span id=\"sec_repeatability_large\"></span>\n",
      "Some more notes on reproducibility"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Summary"
     ]
    }
   ],
   "metadata": {}
  }
 ]
}